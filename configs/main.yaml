#inherit:
#    - models/standard.yaml
#    - experiment.yaml
#    - wandb.yaml

model:
    n_dims: 5
    n_positions: 25
    family: gpt2
    n_embd: 256
    n_layer: 6
    n_head: 4

training:
    task: linear_regression
    data: gaussian
    task_kwargs: {}
    batch_size: 64
    learning_rate: 0.0001
    save_every_steps: 1000
    keep_every_steps: 100000
    max_iters: 5000
    eval_interval: 200
    eval_iters: 20
    curriculum:
        ready_data: False
        dims:
            start: 5
            end: 5
            inc: 1
            interval: 2000
        points:
            start: 25
            end: 25
            inc: 2
            interval: 2000

experiment_conf:
    log_model_weights: True
    logarithmic_scale: False
    show_normality: False
    show_layer_errors: True
    show_embedding_confusion: False
    transform_conf:
        switch_params: null
        duplicate_params: null
        slice_params: null
        average_params: null
        full_backbone_rnn_iters: 1
        no_layernorm_full_backbone_copy: False
        first_n_layers: null
        new_backbone_training: False
        diverge_new_backbone_training: False
        readout2_training: False
        retrain_readout2_iters: 3000
        post_eval: False
        cfm_loss: [0, 0]
        clear_readout2: False
        model_variants: 
          - "modified"
          - "full_backbone + no_final_layer_norm"
        transform_choice: 1
        transform_variants: 
          - "duplicate_layers"
          - "switch_layers"
          - "slice_layers"
          - "average_layers"
    auto_transform_conf:
        permute_bounds_params: [1, 3]
        permute_interval: 10
        permute_model: False
        auto_transform_choice: 1
        auto_transform_variants:
          - "auto_permute_layers"
          - "auto_custom_permute"


out_dir: .\exp_models\linear_regression

wandb:
    name: "linear_regression_toy"
    project: in-context-training
    entity: enemo_ce-warch
    notes: my experiment notes
    log_every_steps: 100
